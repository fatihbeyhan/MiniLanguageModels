model:
  name: mixtral
  d_model: 128
  n_heads: 4
  n_layers: 4
  d_ff: 384
  max_seq_len: 64
  dropout: 0.0
  norm: rmsnorm
  mlp: moe
  pos_encoding: rope
  n_experts: 4
  top_k: 2
  tie_weights: true

training:
  batch_size: 64
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_steps: 100
  max_steps: 10000
  grad_clip: 1.0
  accumulate_grad_batches: 1
  val_check_interval: 0.25
